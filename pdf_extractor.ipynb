{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_and_images(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = []\n",
    "    images = []\n",
    "\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        \n",
    "        # Extract text\n",
    "        text.append(page.get_text())\n",
    "\n",
    "        # Extract images\n",
    "        image_list = page.get_images(full=True)\n",
    "        for image_index, img in enumerate(page.get_images(full=True)):\n",
    "            # get the XREF of the image\n",
    "            xref = img[0]\n",
    "            # extract the image bytes\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            # You can save this image bytes to a file, or keep in memory\n",
    "            images.append(image_bytes)\n",
    "\n",
    "    return text, images\n",
    "\n",
    "pdf_path = '231201 - TDD Red Flag - TRISTAN (MAH Pont Neuf).pdf'\n",
    "text, images = extract_text_and_images(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabula\n",
    "\n",
    "def extract_tables(pdf_path):\n",
    "    # This will return a list of DataFrames, one for each table detected\n",
    "    # You might need to tweak options like pages and multiple_tables based on your PDF\n",
    "    tables = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)\n",
    "    return tables\n",
    "\n",
    "tables = extract_tables(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = '231201 - TDD Red Flag - TRISTAN (MAH Pont Neuf).pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2docx import Converter\n",
    "\n",
    "# Specify the path to your PDF file and the output HTML file\n",
    "pdf_file = '231201 - TDD Red Flag - TRISTAN (MAH Pont Neuf).pdf'\n",
    "html_file = 'output.docx'\n",
    "\n",
    "# Create a Converter object and convert the PDF to HTML\n",
    "cv = Converter(pdf_file)\n",
    "cv.convert(html_file, start=0, end=None, fmt=\"docx\")\n",
    "\n",
    "# Close the converter\n",
    "cv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(pdf_file)\n",
    "loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '\\n'.join([p.page_content for p in pages])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(pdf_file, extract_images=True)\n",
    "pages = loader.load()\n",
    "pages[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "loader = UnstructuredPDFLoader(pdf_file)\n",
    "\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "\n",
    "loader = PDFPlumberLoader(pdf_file)\n",
    "loader.load()\n",
    "file = open('data.txt', 'w+')\n",
    "file.write( '\\n'.join([ p.page_content for p in loader.load()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser   \n",
    "  \n",
    "# opening pdf file \n",
    "parsed_pdf = parser.from_file(pdf_file) \n",
    "  \n",
    "# saving content of pdf \n",
    "# you can also bring text only, by parsed_pdf['text']  \n",
    "# parsed_pdf['content'] returns string  \n",
    "data = parsed_pdf['content']  \n",
    "  \n",
    "# Printing of content  \n",
    "print(data) \n",
    "  \n",
    "# <class 'str'> \n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabula\n",
    "\n",
    "# convert PDF into CSV file\n",
    "tabula.convert_into(pdf_file, \"output.csv\", output_format=\"csv\", pages='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader \n",
    "  \n",
    "# creating a pdf reader object \n",
    "reader = PdfReader(pdf_file) \n",
    "  \n",
    "# printing number of pages in pdf file \n",
    "print(len(reader.pages)) \n",
    "  \n",
    "'\\n\\n'.join([p.extract_text() for p in reader.pages])\n",
    "# # creating a page object \n",
    "# page = reader.pages[1] \n",
    "  \n",
    "# # extracting text from page \n",
    "# print(page.extract_text()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rules:\n",
    "# - Only and only use provided document to answer questions\n",
    "# - if the question is short, ex: a name, date, etc; always and always response shortly\n",
    "# - if the question consist of a paragraph with some missing words/gaps, stick to the sentence and fill the gap. You are allowed to add ONE OR TWO MORE SENTENCE to the answer.\n",
    "# - Your will provide a JSON response as {answer: \"YOUR_ANSWER\"}, DO NOT WRITE ONE MORE WORD THAN THE JSON RESPONSE !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser   \n",
    "\n",
    "def get_document():  \n",
    "    # opening pdf file \n",
    "    parsed_pdf = parser.from_file(pdf_file) \n",
    "    data = parsed_pdf['content']    \n",
    "    print(data.strip()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/api/call_chatgpt', methods=['POST'])\n",
    "def echo():\n",
    "    # Parse the JSON from the incoming request\n",
    "    data = request.get_json()\n",
    "\n",
    "    # Ensure it's a list (or convert it to a list based on your logic)\n",
    "    if not isinstance(data, list):\n",
    "        return jsonify({'error': 'Expected a list'}), 400\n",
    "\n",
    "    # Optionally, manipulate the data\n",
    "    # For example, converting all strings in the list to uppercase\n",
    "    response_data = [s.upper() for s in data]\n",
    "\n",
    "    # Respond with a JSON\n",
    "    return jsonify(response_data)\n",
    "\n",
    "app.run(debug=True, port=62000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embed_model=HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = \"foo bar\"\n",
    "output = embed_model._embed(documents)\n",
    "[o  for out in  output for o in out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ali.karooni\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "establishing the connection...\n",
      "initialiing the embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create db...\n",
      "Current databases:  ['default', 'vector_db']\n",
      "create collection...\n",
      "building index...\n"
     ]
    }
   ],
   "source": [
    "from vector_db import Vector_DB\n",
    "import numpy as np\n",
    "\n",
    "vector_db = Vector_DB()\n",
    "vector_db.preparing_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_inset = [\n",
    "\"PARAGRAPH:^ This report is intended for the addressee only and third parties are not permitted to rely on the contents without the express permission of Savills France.\",\n",
    "\"PARAGRAPH:^Survey Limitations\",\n",
    "\"PARAGRAPH:^This [Select report type] and our inspection have been undertaken and prepared in accordance with our Standard Survey Limitations (Commercial Building Surveys), which is attached as an appendix. \",\n",
    "\"PARAGRAPH:^No opening up or testing of the building fabric or building services installations has been undertaken unless stated to the contrary in this report.\",\n",
    "\"PARAGRAPH:^Given that you are acquiring the freehold interest in the property, our inspection and report concentrate on significant items of disrepair.  Minor disrepair items are therefore excluded from this report\",\n",
    "\"PARAGRAPH:^Inspection Details\",\n",
    "\"PARAGRAPH:^This inspection was carried out by [insert surveyor name], Savills on [insert date] and we were accompanied by the [e.g. property manager, etc.]\",\n",
    "\"PARAGRAPH:^The estate was [not] fully accessible. The inspection was undertaken on a visual basis, without proving materials or destructive investigations. Pictures are used to clarify the text. \",\n",
    "\"PARAGRAPH:^The weather was [e.g. heavy rainfall, clear and cool, etc.] for the duration of the inspection.  \",\n",
    "\"PARAGRAPH:^The elevation facing [name of street] is deemed to face [e.g. north, south, etc.], with all other directional references following this orientation.\",\n",
    "\"PARAGRAPH:^Documents\",\n",
    "\"PARAGRAPH:^We have [not (delete if not applicable)] been provided with access to the online data room (Espace Notarial). The documentation reviewed has been referred to as necessary in the report. The documentation is [comprehensive and complete / largely comprehensive and complete / not complete]. The “Q&A” (Questions & Answers) option within the data room has [not (delete if not applicable)] been made available for use during the due diligence process [and our questions have been posted directly (delete if not applicable)]. [The list of documents that have not been provided in the data room is enumerated in Appendix 1.]\",\n",
    "\"PARAGRAPH:^Property Description\",\n",
    "\"PARAGRAPH:^General Description\",\n",
    "\"PARAGRAPH:^The subject property is [a/an] [insert age] year[s] old [warehouse/office] building situated [on ZAC] [insert ZAC name] in [Municipality] in the department of [insert Department name] ([insert number of department]). It is located around [insert distance]km [geographical location (e.g. northwest)] from [Municipality name of closest main town] city centre and [insert distance]km to the [geographical location (e.g. northwest)] of [insert name of other known main city for reference] The building is [not] accessible by public transport.\",\n",
    "\"PARAGRAPH:^The goods delivery to the warehouse is along the [front, rear etc.] elevation with a [insert number]m deep truck yard to the front of the loading bays according to [onsite measurement/aerial plans]. The office accommodation is on the [front elevation between cells 2 and 3, etc.]. There [is/are] [one/two] battery charging room[s] located to the [insert location] (see aerial plan below).\",\n",
    "\"PARAGRAPH:^The site is bounded by [insert street/avenue, etc., name] to the [geographical location (e.g. northwest)], [insert street/avenue, etc. 2, name] to the [geographical location (e.g. northwest)], [insert other streets or adjacent buildings, etc.]. The edge of the property is outlined in red in the aerial photograph below. \",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_to_inset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m embedded_text \u001b[38;5;241m=\u001b[39m [vector_db\u001b[38;5;241m.\u001b[39mtext_embedding(data) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata_to_inset\u001b[49m]\n\u001b[0;32m      2\u001b[0m vector_db\u001b[38;5;241m.\u001b[39minsert_into_vector_db(data_to_inset, embedded_text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_to_inset' is not defined"
     ]
    }
   ],
   "source": [
    "embedded_text = [vector_db.text_embedding(data) for data in data_to_inset]\n",
    "vector_db.insert_into_vector_db(data_to_inset, embedded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[\"id: 448606180286283284, distance: 13.459665298461914, entity: {\\'text\\': \\'PARAGRAPH:^ This report is intended for the addressee only and third parties are not permitted to rely on the contents without the express permission of Savills France.\\'}\"]']\n",
      "Closest match ID: 448606180286283284, Text: PARAGRAPH:^ This report is intended for the addressee only and third parties are not permitted to rely on the contents without the express permission of Savills France., Similarity: 0.06915789400093311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id: 448606180286283284, distance: 13.459665298461914, entity: {'text': 'PARAGRAPH:^ This report is intended for the addressee only and third parties are not permitted to rely on the contents without the express permission of Savills France.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db.query(\"(PARAGRAPH:^This report is intended for the addressee only and third parties are not permitted to rely on the contents without the express permission of Savills France.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DPRContextEncoder\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "\n",
    "# Load the DPR Context Encoder model\n",
    "model = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "\n",
    "# Example text input\n",
    "text = \"Your example text here\"\n",
    "\n",
    "# Tokenize the input text\n",
    "input_tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Get embeddings from the model\n",
    "embeddings = model(**input_tokens).pooler_output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
